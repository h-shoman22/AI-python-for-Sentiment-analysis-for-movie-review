# -*- coding: utf-8 -*-
"""AI Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JPfsO5JhMSWOm-xNVD4u3ZVWkycRqZxy
"""

# Import libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pickle
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from keras import regularizers

# Load the IMDB movie reviews dataset
movie_reviews_data = pd.read_csv("IMDB Dataset.csv")
movie_reviews_data.head()

# Exploratory Data Analysis (EDA)
print("Shape of the dataset:", movie_reviews_data.shape)
print("Null values in the dataset:\n", movie_reviews_data.isnull().sum())
print("Summary statistics:\n", movie_reviews_data.describe())
print("Information about the dataset:\n", movie_reviews_data.info())
print("Unique values in 'sentiment' column:", movie_reviews_data['sentiment'].unique())
print("Value counts in 'sentiment' column:\n", movie_reviews_data['sentiment'].value_counts())

# Encode the sentiment labels
label_ecdr = LabelEncoder()
movie_reviews_data['sentiment'] = label_ecdr.fit_transform(movie_reviews_data['sentiment'])

movie_reviews_data.head()

x = movie_reviews_data['review']
y = movie_reviews_data['sentiment']

# Text Preprocessing
nltk.download('stopwords')

porter_stemmer = PorterStemmer()
corpus = []

# Cleaning and preprocessing each review
for i in range(len(x)):
    print(i)
    review = BeautifulSoup(movie_reviews_data['review'][i], 'html.parser').get_text()  # Use BeautifulSoup to remove HTML tags
    review = re.sub("[^a-zA-Z]", " ", review)
    review = review.lower()
    review = review.split()
    review = [porter_stemmer.stem(word) for word in review if word not in set(stopwords.words("english"))]
    review = " ".join(review)
    corpus.append(review)

# Split the data into training (80%), validation (20%), and testing (20%) sets
x_train, x_test, y_train, y_test = train_test_split(corpus, movie_reviews_data['sentiment'], test_size=0.2, random_state=101)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=101)

cvectorizer = TfidfVectorizer(max_features=2000)
x_train = cvectorizer.fit_transform(x_train).toarray()
x_val = cvectorizer.transform(x_val).toarray()
x_test = cvectorizer.transform(x_test).toarray()

# Create a simpler neural network with dropout layers and L2 regularization
mdl = Sequential()
mdl.add(Dense(units=256, activation='relu', input_dim=x_train.shape[1], kernel_regularizer=regularizers.l2(0.01)))
mdl.add(Dropout(0.5))
mdl.add(Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
mdl.add(Dropout(0.5))
mdl.add(Dense(units=1, activation='sigmoid'))

# Model Compiler
mdl.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Apply early stopping during model training
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Model Training implementing early stopping
hist = mdl.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[early_stopping])

# Plotting accuracy
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plotting loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Testing Evaluation of model on the test set
predtest_prob = mdl.predict(x_test)
predtest = (predtest_prob > 0.5).astype(int)

print("Test Accuracy:", accuracy_score(y_test, predtest))
print("Confusion Matrix (Test):\n", confusion_matrix(y_test, predtest))
print("Classification Report (Test):\n", classification_report(y_test, predtest))

# Saves the model and vectorizer
pickle.dump(cvectorizer, open("count-Vectorizer.pkl", "wb"))
pickle.dump(mdl, open("Movies_Reviews_Classification.pkl", "wb"))

# Loads the model and vectorizer
save_cvectorizer = pickle.load(open('count-Vectorizer.pkl', 'rb'))
load_model = pickle.load(open('Movies_Reviews_Classification.pkl', 'rb'))

def test_model(sentence):
    sentiment = save_cvectorizer.transform([sentence]).toarray()
    pred_prob = load_model.predict(sentiment)
    result = (pred_prob > 0.5).astype(int)[0][0]
    if result == 1:
        return 'Positive review'
    else:
        return 'Negative review'
# ALL REVIEWS BELOW POSITIVE/NEGATIVE

sentiment = 'This is a good movie'
result = test_model(sentiment)
print(result)

sentiment = 'This is a good movie however i didnt like it entirely much.'
res = test_model(sentiment)
print(result)

sentiment = 'This is a an okay movie.'
result = test_model(sentiment)
print(result)

sentiment = 'i have wasted money on a terrible movie!'
result = test_model(sentiment)
print(result)

sentiment = 'this is a terrible movie!'
result = test_model(sentiment)
print(result)

sentiment = 'this is a bad movie.'
result = test_model(sentiment)
print(result)